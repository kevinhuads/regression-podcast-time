{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a66093",
   "metadata": {},
   "source": [
    "# 2. Podcast Listening Time - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fef8c",
   "metadata": {},
   "source": [
    "We have done the Exploratory Data Analysis in the [Last Notebook](1_Podcast_EDA.ipynb), now it's time to actually start to model the Listening Time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d18fcc",
   "metadata": {},
   "source": [
    "# Base Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8a0f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=r\".*A worker stopped while some jobs were given to the executor.*\",\n",
    "    category=UserWarning, module=r\"joblib[.]externals[.]loky[.]process_executor\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Using `tqdm.autonotebook.tqdm` in notebook mode.*\",\n",
    "    module=r\"tqdm_joblib\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import set_config\n",
    "from podcast_functions import *\n",
    "from podcast_models import *\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score,root_mean_squared_error\n",
    "\n",
    "\n",
    "result_folder = \"Results\"\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "set_config(transform_output=\"pandas\")   \n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(r\"Data/train.csv\")\n",
    "df_test = pd.read_csv(r\"Data/test.csv\") \n",
    "\n",
    "target = \"Listening_Time_minutes\"\n",
    "X_cat = [\"Podcast_Name\",\"Publication_Day\", \"Publication_Time\", \"Episode_Title\",\"Episode_Sentiment\"]\n",
    "X_num = [\"Episode_Length_minutes\",\"Guest_Popularity_percentage\",\"Number_of_Ads\",\"Host_Popularity_percentage\"]\n",
    "\n",
    "# Set categorical columns to 'category' dtype\n",
    "for col in X_cat:\n",
    "    df_train[col] = df_train[col].astype('category')\n",
    "    df_test[col]  = df_test[col].astype('category')\n",
    "    \n",
    "X = df_train[X_cat + X_num]\n",
    "y = df_train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab6b45",
   "metadata": {},
   "source": [
    "The goal is to minimize the RMSE since it's the metric used in the competition. We already explained in the EDA the steps in the preprocessors aiming at cleaning the Data (removing outliers, capping data...).\n",
    "\n",
    "The other steps in the preprocessor vary depending on the algorithms as some of them need imputing missing values, encoding categorical variables or scaling numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb117832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = {\n",
    "    \"base\": make_preprocessor(X_num, X_cat),\n",
    "    \"imp\": make_preprocessor(X_num, X_cat, impute=True),\n",
    "    \"imp_onehot\": make_preprocessor(X_num, X_cat, onehot=True, impute=True, sparse_output=True, scaler=True),\n",
    "    \"imp_dense\": make_preprocessor(X_num, X_cat, onehot=True, impute=True, sparse_output=False),\n",
    "    \"dense\": make_preprocessor(X_num, X_cat, onehot=True, sparse_output=False),\n",
    "    \"imp_dense_sc\": make_preprocessor(X_num, X_cat, onehot=True, impute=True, sparse_output=False, scaler=True),\n",
    "    \"imp_ordinal\": make_preprocessor(X_num, X_cat, impute=True, ordinal=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34442626",
   "metadata": {},
   "source": [
    "We are going to use a diverse set of estimators to capture the maximum of variance possible. Each pipeline is tune with cross-validation and model-appropriate search strategies:\n",
    "- Fast and Strong baselines Gradient Boosting Models: **LightGBM, XGBoost, CatBoost and HistGradientBoosting**\n",
    "- Bagging / Randomized trees: **Random Forest and ExtraTrees**\n",
    "- Purely Additive Boosting : **Explainable Boosting Machine**\n",
    "- Linear and Distance/Margin models : **ElasticNet, k NN and SVR (RBF)**\n",
    "- Neural-based tabular models : **Shallow MLP and TabNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10aa273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lgbm\n",
      "Loading xgb\n",
      "Loading cat\n",
      "Loading hgb\n",
      "Loading rf\n",
      "Loading et\n",
      "Loading ebm\n",
      "Loading enet\n",
      "Loading knn\n",
      "Loading svr\n",
      "Loading nn\n",
      "Loading tab\n"
     ]
    }
   ],
   "source": [
    "# --- Unified config list ---\n",
    "configs = [\n",
    "    (\"lgbm\",run_lightGBM,      {\"preprocessor\": pre[\"base\"]}),\n",
    "    (\"xgb\", run_XGBoost,       {\"preprocessor\": pre[\"base\"]}),\n",
    "    (\"cat\", run_CatBoost,      {\"preprocessor\": pre[\"base\"], \"X_cat\": X_cat}),\n",
    "    (\"hgb\", run_HGB,           {\"preprocessor\": pre[\"imp_dense\"]}),\n",
    "    (\"rf\",  run_randomForest,  {\"preprocessor\": pre[\"dense\"]}),\n",
    "    (\"et\",  run_extraTrees,    {\"preprocessor\": pre[\"imp_dense\"], \"n_candidates\": 20, \"n_jobs\": 8}),\n",
    "    (\"ebm\", run_EBM,           {\"preprocessor\": pre[\"imp\"]}),\n",
    "    (\"enet\",run_ElasticNet,    {\"preprocessor\": pre[\"imp_onehot\"], \"n_jobs\": 16}),\n",
    "    (\"knn\", run_KNN,           {\"preprocessor\": pre[\"imp_dense_sc\"], \"n_jobs\": 16}),\n",
    "    (\"svr\", run_SVR,           {\"preprocessor\": pre[\"imp_dense_sc\"],  \"n_candidates\": 25, \"n_jobs\": 8}),\n",
    "    (\"nn\",  run_NeuralNetwork, {\"preprocessor\": pre[\"imp_dense\"], \"n_jobs\": 8}),\n",
    "    (\"tab\", run_tabnet,        {\"X_num\": X_num, \"X_cat\": X_cat, \"preprocessor\": pre[\"imp_ordinal\"]}),\n",
    "]\n",
    "\n",
    "# --- Run models or load them from disk ---\n",
    "model_folder = os.path.join(result_folder, \"models\")\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "overwrite_model = False\n",
    "models = {}\n",
    "\n",
    "for key, func, kw in configs:\n",
    "    \n",
    "    model_path = os.path.join(model_folder, f\"models_{key}.joblib\")\n",
    "    if overwrite_model or not os.path.exists(model_path):\n",
    "        print(f\"Running {key}\")\n",
    "        search = func(X=X, y=y, **kw)\n",
    "        joblib.dump(search, model_path, compress=3)\n",
    "        models[key] = search\n",
    "    else:\n",
    "        print(f\"Loading {key}\")\n",
    "        models[key] = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# --- Write Predictions ---\n",
    "pred_folder = os.path.join(result_folder, \"predictions\")\n",
    "os.makedirs(pred_folder, exist_ok=True)\n",
    "\n",
    "for model_name, search in models.items():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    write_predictions(\n",
    "        df=df_test,\n",
    "        model=search.best_estimator_,\n",
    "        features=X_cat + X_num,\n",
    "        target=target,\n",
    "        path= os.path.join(pred_folder, f\"predictions_{model_name}.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad100d7",
   "metadata": {},
   "source": [
    "We have a good idea about the ranking of our different models, even though we have to be careful as these are the results of the Cross Validation, we will get the *real* results after we submit our predictions to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c3f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM CV RMSE: 12.689524 / R2 : 0.781362\n",
      "XGB CV RMSE: 12.738678 / R2 : 0.779665\n",
      "CAT CV RMSE: 12.980260 / R2 : 0.771229\n",
      "HGB CV RMSE: 13.010592 / R2 : 0.770158\n",
      "RF CV RMSE: 12.637614 / R2 : 0.783147\n",
      "ET CV RMSE: 12.692488 / R2 : 0.781260\n",
      "EBM CV RMSE: 13.074538 / R2 : 0.767893\n",
      "ENET CV RMSE: 13.346651 / R2 : 0.758131\n",
      "KNN CV RMSE: 12.995897 / R2 : 0.770677\n",
      "SVR CV RMSE: 13.266125 / R2 : 0.761041\n",
      "NN CV RMSE: 13.273978 / R2 : 0.760758\n",
      "TAB CV RMSE: 13.255676 / R2 : 0.761417\n"
     ]
    }
   ],
   "source": [
    "# --- Write Predictions ---\n",
    "cv_folder = os.path.join(result_folder, \"cross_validation\")\n",
    "os.makedirs(cv_folder, exist_ok=True)\n",
    "\n",
    "overwrite_cv = False\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "oof_list = []\n",
    "\n",
    "for model_name, search in models.items():\n",
    "    np_path = os.path.join(cv_folder, f\"cv_{model_name}.npy\")\n",
    "    best = search.best_estimator_\n",
    "    if overwrite_cv or not os.path.exists(np_path):\n",
    "        oof_pred = cross_val_predict(best, X, y, cv=cv, n_jobs=3)\n",
    "        np.save(np_path, oof_pred)\n",
    "    else:\n",
    "        oof_pred = np.load(np_path)\n",
    "        \n",
    "    oof_r2 = r2_score(y, oof_pred)\n",
    "    oof_rmse = root_mean_squared_error(y, oof_pred)\n",
    "    \n",
    "    oof_list.append(oof_pred)\n",
    "    r2_list.append(oof_r2)\n",
    "    rmse_list.append(oof_rmse)\n",
    "    print(f\"{model_name.upper()} CV RMSE: {oof_rmse:.6f} / R2 : {oof_r2:.6f}\")\n",
    "    \n",
    "df_rmse = pd.DataFrame({\n",
    "    'model': list(models.keys()),\n",
    "    'rmse': rmse_list,\n",
    "    'r2': r2_list \n",
    "})\n",
    "\n",
    "matrix_oof = np.vstack(oof_list)\n",
    "matrix_oof_T = matrix_oof.T\n",
    "df_oof = pd.DataFrame(matrix_oof_T)\n",
    "df_oof.columns = list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5c50a",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cc684",
   "metadata": {},
   "source": [
    "After identifying strong individual learners, we build a **StackingRegressor** where the final estimator is LightGBM as it allows to\n",
    "- Weight the different mistakes of the single models\n",
    "- Blend high-bias and high-variance learners to reduce RMSE without over-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2138b72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overwrite_stack = False\n",
    "stack_path = os.path.join(model_folder, \"models_stack.joblib\")\n",
    "\n",
    "if overwrite_stack or not os.path.exists(stack_path):\n",
    "    stack = StackingRegressor(\n",
    "        estimators=[(model_name, search.best_estimator_) for model_name, search in models.items()],\n",
    "        final_estimator=LGBMRegressor(\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=3,        \n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            random_state=SEED,\n",
    "            verbosity=-1\n",
    "        ),\n",
    "        cv=cv,\n",
    "        n_jobs=1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    with tqdm_joblib(tqdm(total=2 * len(models), desc=\"GridSearch Stacking\")):\n",
    "        stack.fit(X, y)\n",
    "    joblib.dump(stack,stack_path , compress=3)\n",
    "else:\n",
    "    stack = joblib.load(stack_path)\n",
    "    \n",
    "write_predictions(\n",
    "    df=df_test,model=stack,features=X_cat + X_num,\n",
    "    target=target,path= os.path.join(pred_folder, \"predictions_stack.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea328523",
   "metadata": {},
   "source": [
    "#### Stack Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0392ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_stack_cv = False\n",
    "stack_cv_path = os.path.join(cv_folder, \"cv_stack.npy\")\n",
    "\n",
    "if overwrite_stack_cv or not os.path.exists(stack_cv_path):\n",
    "    with tqdm_joblib(tqdm(total=cv.get_n_splits(), desc=\"GridSearch Cross Val Predict\")):\n",
    "        oof_stack = cross_val_predict(stack, X, y,cv=cv, n_jobs=2,method='predict',verbose=10)\n",
    "        np.save(stack_cv_path, oof_stack)\n",
    "else:\n",
    "    oof_stack = np.load(stack_cv_path)\n",
    "    \n",
    "rmse_stack = root_mean_squared_error(y, oof_stack)\n",
    "r2_stack   = r2_score(y, oof_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the results data frames with the stack\n",
    "df_rmse.loc[len(df_rmse)] = [\"stack\", rmse_stack, r2_stack]\n",
    "df_oof[\"stack\"] = oof_stack\n",
    "df_rmse.to_csv(os.path.join(result_folder, \"cv_score.csv\"), index=False)  \n",
    "df_oof.to_csv(os.path.join(result_folder, \"cv_oof.csv\"), index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338a46f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>12.689524</td>\n",
       "      <td>0.781362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb</td>\n",
       "      <td>12.738678</td>\n",
       "      <td>0.779665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>12.980260</td>\n",
       "      <td>0.771229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hgb</td>\n",
       "      <td>13.010592</td>\n",
       "      <td>0.770158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>12.637614</td>\n",
       "      <td>0.783147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>et</td>\n",
       "      <td>12.692488</td>\n",
       "      <td>0.781260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ebm</td>\n",
       "      <td>13.074538</td>\n",
       "      <td>0.767893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>enet</td>\n",
       "      <td>13.346651</td>\n",
       "      <td>0.758131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>knn</td>\n",
       "      <td>12.995897</td>\n",
       "      <td>0.770677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>svr</td>\n",
       "      <td>13.266125</td>\n",
       "      <td>0.761041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nn</td>\n",
       "      <td>13.273978</td>\n",
       "      <td>0.760758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tab</td>\n",
       "      <td>13.255676</td>\n",
       "      <td>0.761417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stack</td>\n",
       "      <td>12.451470</td>\n",
       "      <td>0.789488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       rmse        r2\n",
       "0    lgbm  12.689524  0.781362\n",
       "1     xgb  12.738678  0.779665\n",
       "2     cat  12.980260  0.771229\n",
       "3     hgb  13.010592  0.770158\n",
       "4      rf  12.637614  0.783147\n",
       "5      et  12.692488  0.781260\n",
       "6     ebm  13.074538  0.767893\n",
       "7    enet  13.346651  0.758131\n",
       "8     knn  12.995897  0.770677\n",
       "9     svr  13.266125  0.761041\n",
       "10     nn  13.273978  0.760758\n",
       "11    tab  13.255676  0.761417\n",
       "12  stack  12.451470  0.789488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b15c4",
   "metadata": {},
   "source": [
    "Now that our models are ready, we are going to analyze them in the [Final Analysis Notebook](3_Podcast_Final_Analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
